version: '3.8'

services:
  ollama:
    image: ollama/ollama:${OLLAMA_DOCKER_TAG-latest}
    container_name: ollama
    volumes:
      - $PWD/ollama:/root/.ollama
    devices:
      # Uncomment the appropriate lines depending on your GPU
      - /dev/kfd:/dev/kfd         # For AMD GPUs
      - /dev/dri:/dev/dri         # For AMD GPUs
      #- /dev/nvidia0:/dev/nvidia0  # For NVIDIA GPUs
      #- /dev/nvidiactl:/dev/nvidiactl  # For NVIDIA GPUs
      #- /dev/nvidia-uvm:/dev/nvidia-uvm  # For NVIDIA GPUs
      #- /dev/nvidia-uvm-tools:/dev/nvidia-uvm-tools  # For NVIDIA GPUs
    environment:
      # Uncomment the appropriate line depending on your GPU
      - HSA_OVERRIDE_GFX_VERSION=${HSA_OVERRIDE_GFX_VERSION-11.0.0}  # For AMD GPUs
      #- NVIDIA_VISIBLE_DEVICES=all  # For NVIDIA GPUs
      #- NVIDIA_DRIVER_CAPABILITIES=compute,utility  # For NVIDIA GPUs
    tty: true
    restart: unless-stopped
    pull_policy: always

  open-webui:
    build:
      context: .
      args:
        OLLAMA_BASE_URL: '/ollama'
      dockerfile: Dockerfile
    image: ghcr.io/open-webui/open-webui:${WEBUI_DOCKER_TAG-main}
    container_name: open-webui
    volumes:
      - $PWD/open-webui:/app/backend/data
    depends_on:
      - ollama
    ports:
      - ${OPEN_WEBUI_PORT-3000}:8080
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
      - WEBUI_SECRET_KEY
    extra_hosts:
      - host.docker.internal:host-gateway
    restart: unless-stopped

volumes:
  ollama: {}
  open-webui: {}
